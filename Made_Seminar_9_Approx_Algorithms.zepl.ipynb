{
  "metadata": {
    "name": "made-seminar",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df \u003d spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"/notebook/tripadvisor_hotel_reviews.csv\")"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df.show"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.{HashingTF, IDF, RegexTokenizer}\n\n\nval preprocessingPipe \u003d new Pipeline()\n    .setStages(Array(\n        new RegexTokenizer()\n        .setInputCol(\"Review\")\n        .setOutputCol(\"tokenized\")\n        .setPattern(\"\\\\W+\"),\n        new HashingTF()\n            .setInputCol(\"tokenized\")\n            .setOutputCol(\"tf\")\n            .setBinary(true)\n            .setNumFeatures(1000),\n        new HashingTF()\n            .setInputCol(\"tokenized\")\n            .setOutputCol(\"tf2\")\n            .setNumFeatures(1000),\n        new IDF()\n            .setInputCol(\"tf2\")\n            .setOutputCol(\"tfidf\")\n    ))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val Array(train, test) \u003d df.randomSplit(Array(0.8, 0.2))\n\n\nval pipe \u003d preprocessingPipe.fit(train)\n\nval trainFeatures \u003d pipe.transform(train).cache()\nval testFeatures \u003d pipe.transform(test)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "trainFeatures.show"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val testFeaturesWithIndex \u003d testFeatures.withColumn(\"id\", monotonicallyIncreasingId()).cache()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "testFeaturesWithIndex.show"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.feature.MinHashLSH\n\n\nval mh \u003d new MinHashLSH()\n    .setInputCol(\"tf\")\n    .setOutputCol(\"buckets\")\n    .setNumHashTables(3)\n\nval mhModel \u003d mh.fit(trainFeatures)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "mhModel.transform(trainFeatures).select(col(\"buckets\")).show(truncate\u003dfalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val neighbors \u003d mhModel.approxSimilarityJoin(trainFeatures, testFeaturesWithIndex, 0.8)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "neighbors.printSchema"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "neighbors.show"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val predictions \u003d neighbors\n    .withColumn(\"similarity\", (lit(1) - col(\"distCol\")))\n    .groupBy(\"datasetB.id\")\n    .agg((sum(col(\"similarity\") * col(\"datasetA.Rating\")) / sum(col(\"similarity\"))).as(\"predict\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val forMetric \u003d testFeaturesWithIndex.join(predictions, Seq(\"id\"))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.evaluation.RegressionEvaluator\n\nval metrics \u003d new RegressionEvaluator()\n    .setLabelCol(\"Rating\")\n    .setPredictionCol(\"predict\")\n    .setMetricName(\"rmse\")"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "metrics.evaluate(forMetric)"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val results \u003d Array.range(3, 16, 2).map(numHashes \u003d\u003e {\n    val mh \u003d new MinHashLSH()\n        .setInputCol(\"tf\")\n        .setOutputCol(\"buckets\")\n        .setNumHashTables(numHashes)\n        .fit(trainFeatures)\n    \n    val neighbors \u003d mh.approxSimilarityJoin(trainFeatures, testFeaturesWithIndex, 0.7)\n    \n    val predictions \u003d neighbors\n        .withColumn(\"similarity\", (lit(1) - col(\"distCol\")))\n        .groupBy(\"datasetB.id\")\n        .agg(\n            (sum(col(\"similarity\") * col(\"datasetA.Rating\")) / sum(col(\"similarity\"))).as(\"predict\"),\n            count(\"datasetA.Rating\").as(\"numNeighbors\")\n        )\n    \n    val forMetric \u003d testFeaturesWithIndex.join(predictions, Seq(\"id\"))\n    \n    val meanNumNeighbors \u003d forMetric.select(avg(\"numNeighbors\")).collect.head(0)\n    \n    val metric \u003d metrics.evaluate(forMetric)\n    \n    val res \u003d (numHashes, metric, meanNumNeighbors)\n    println(res)\n    res\n})"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(results.map(x \u003d\u003e (x._1, x._2)).toList.toDF(\"numHashes\", \"rmse\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.feature.BucketedRandomProjectionLSH\n\n\nval mh \u003d new BucketedRandomProjectionLSH()\n    .setInputCol(\"tfidf\")\n    .setOutputCol(\"brpBuckets\")\n    .setBucketLength(5)\n    .setNumHashTables(3)\n\nval mhModel \u003d mh.fit(trainFeatures)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val euqlidNeigh \u003d mhModel.approxSimilarityJoin(trainFeatures, testFeaturesWithIndex, 10)\n\neuqlidNeigh.show"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val predictions \u003d euqlidNeigh\n    .withColumn(\"similarity\", (lit(1) / (col(\"distCol\") + lit(0.0000001))))\n    .groupBy(\"datasetB.id\")\n    .agg((sum(col(\"similarity\") * col(\"datasetA.Rating\")) / sum(col(\"similarity\"))).as(\"predict\"))\n\nval forMetric \u003d testFeaturesWithIndex.join(predictions, Seq(\"id\"))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "metrics.evaluate(forMetric)"
    }
  ]
}